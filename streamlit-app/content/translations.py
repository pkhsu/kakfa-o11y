def get_text(language, key):
    # Fallback to English if the key is not found in the selected language
    return TRANSLATIONS.get(language, {}).get(key) or TRANSLATIONS.get("English", {}).get(key, f"Missing translation for {key}")

TRANSLATIONS = {
    "English": {
        # app.py
        "app_title": "Kafka APM & Observability Demo",
        "welcome_header": "Welcome to the Kafka Observability Demo!",
        "welcome_intro": """
This project is designed to showcase how to build a comprehensive monitoring, logging, and tracing solution for Kafka-based applications using the **OpenTelemetry** and **Grafana** ecosystems.

ğŸ‘ˆ **Select a page from the sidebar to get started.**
""",
        "project_goals_header": "ğŸš€ Project Goals",
        "project_goals_content": """
- **End-to-End Observability**: Achieve full coverage of metrics, logs, and traces from producer to consumer.
- **Polyglot Support**: Demonstrate how to monitor applications written in different languages (Java, Python, Go).
- **Interactive Experience**: Provide a Streamlit application for live monitoring, scenario simulation, and data exploration.
- **Best Practices**: Apply observability design patterns like centralized collection and structured logging.
""",
        "ready_message": "The project is ready to go! All services should be running in Docker.",
        "before_you_begin_header": "âœ¨ Before You Begin",
        "before_you_begin_content": "Make sure you have read the project's `README.md` and have run `docker-compose up -d` to start all services.",
        "language_select": "Choose a language:",

        # 1_Architecture.py
        "arch_title": "ğŸ—ï¸ System Architecture",
        "arch_overview_header": "Observability Architecture Overview",
        "arch_principles_header": "ğŸ¯ Key Architecture Principles",
        "arch_principle_central": "Centralized Collection",
        "arch_principle_central_desc": "Single OpenTelemetry Collector for all telemetry data",
        "arch_principle_vendor": "Vendor Neutrality",
        "arch_principle_vendor_desc": "Standards-based OpenTelemetry implementation",
        "arch_principle_scale": "Scalability",
        "arch_principle_scale_desc": "Horizontally scalable components with persistent storage",
        "arch_principle_security": "Security",
        "arch_principle_security_desc": "Configurable authentication and encryption support",
        "arch_principle_ha": "High Availability",
        "arch_principle_ha_desc": "Resilient design with health checks and automatic recovery",
        "data_flow_header": "ğŸŒŠ Data Flow Architecture",
        "perf_header": "âš¡ Performance Characteristics",
        "perf_component": "Component",
        "perf_throughput": "Expected Throughput",
        "perf_latency": "Latency (P95)",
        "perf_memory": "Memory Usage",

        # 0_Tutorial_Introduction.py
        "intro_title": "Tutorial Introduction",
        "intro_markdown": """
This interactive tutorial is designed to guide you through the **Kafka APM & Observability Solution Demo**. 
You will learn about the system architecture, technology stack, and how to use the monitoring tools to troubleshoot performance issues.
""",
        "intro_info": "Please proceed through the pages in the sidebar to complete the tutorial. We recommend starting with the 'Architecture' page.",
        "intro_structure_header": "Tutorial Structure",
        "intro_structure_content": """
- **Architecture**: Understand the overall system design.
- **Tech Stack**: Learn about the technologies used.
- **Live Dashboard**: See the Grafana dashboard in action.
- **Demo Scenarios**: Run interactive scenarios to simulate real-world situations.
- **And more...**
""",
        "intro_prereq_header": "Prerequisites",
        "intro_prereq_content": """
- **Docker**: Ensure Docker and Docker Compose are installed and running.
- **Project Files**: You should have the project files cloned to your local machine.
- **Basic Kafka Knowledge**: Familiarity with basic Kafka concepts is helpful but not required.
""",

        # 2_Tech_Stack.py
        "tech_stack_title": "ğŸ”§ Technology Stack",
        "tech_app_layer": "ğŸ“± Application Instrumentation Layer",
        "tech_component": "Component",
        "tech_technology": "Technology",
        "tech_purpose": "Purpose",
        "tech_features": "Key Features",
        "tech_java_apps": "Java Apps",
        "tech_python_apps": "Python Apps",
        "tech_go_apps": "Go Apps",
        "tech_java_agent": "OpenTelemetry Java Agent",
        "tech_python_sdk": "OpenTelemetry Python SDK",
        "tech_go_sdk": "OpenTelemetry Go SDK",
        "tech_auto_instrument": "Auto-instrumentation",
        "tech_manual_instrument": "Manual instrumentation",
        "tech_java_features": "Zero-code instrumentation, JVM metrics, Kafka client tracing",
        "tech_python_features": "Custom metrics, structured logging, async support",
        "tech_go_features": "High-performance tracing, custom metrics, low overhead",
        "tech_streaming_platform": "ğŸ“¨ Message Streaming Platform",
        "tech_kafka": "Apache Kafka",
        "tech_zookeeper": "Zookeeper",
        "tech_jmx": "JMX Exporters",
        "tech_confluent": "Confluent Platform",
        "tech_apache_zookeeper": "Apache Zookeeper",
        "tech_prometheus_jmx": "Prometheus JMX Exporter",
        "tech_broker": "Message Broker",
        "tech_coordination": "Coordination Service",
        "tech_metrics_collection": "Metrics Collection",
        "tech_kafka_features": "High throughput, durability, partition-based scaling",
        "tech_zookeeper_features": "Cluster coordination, configuration management",
        "tech_jmx_features": "Broker metrics, JVM stats, topic metrics",
        "tech_obs_infra": "ğŸ‘ï¸ Observability Infrastructure",
        "tech_otel_collector": "OpenTelemetry Collector",
        "tech_prometheus": "Prometheus",
        "tech_loki": "Loki",
        "tech_tempo": "Tempo",
        "tech_grafana": "Grafana",
        "tech_otel_contrib": "OTEL Collector Contrib",
        "tech_prometheus_tsdb": "Prometheus TSDB",
        "tech_grafana_loki": "Grafana Loki",
        "tech_grafana_tempo": "Grafana Tempo",
        "tech_grafana_viz": "Grafana",
        "tech_pipeline": "Telemetry Pipeline",
        "tech_metrics_storage": "Metrics Storage",
        "tech_log_aggregation": "Log Aggregation",
        "tech_trace_storage": "Trace Storage",
        "tech_visualization": "Visualization",
        "tech_otel_features": "Data collection, processing, routing, export",
        "tech_prometheus_features": "Time-series database, PromQL queries, alerting",
        "tech_loki_features": "Log indexing, LogQL queries, label-based organization",
        "tech_tempo_features": "Distributed tracing, trace correlation, sampling",
        "tech_grafana_features": "Dashboards, alerting, data exploration",

        # 3_Live_Dashboard.py
        "dashboard_title": "ğŸ“Š Live System Dashboard",
        "dashboard_header": "Real-time Monitoring & Metrics",
        "auto_refresh": "Auto-refresh (30s)",
        "service_status": "ğŸš¥ Service Status",
        "prometheus_status": "Prometheus",
        "grafana_status": "Grafana",
        "otel_collector_status": "OTel Collector",
        "kafka_status": "Kafka",
        "running": "Running",
        "down": "Down",
        "key_metrics": "ğŸ“ˆ Key Metrics",
        "throughput_tab": "Message Throughput",
        "latency_tab": "Latency Metrics",
        "resources_tab": "System Resources",
        "errors_tab": "Error Rates",
        "producer_throughput_header": "ğŸ“¤ Producer Throughput",
        "python_producer_service": "Python Producer",
        "go_producer_service": "Go Producer",
        "current_throughput_title": "Current Throughput",
        "no_throughput_data": "No throughput data available",
        "no_producer_metrics": "No producer metrics found",
        "consumer_lag_header": "ğŸ“¥ Consumer Lag",
        "consumer_lag_title": "Consumer Lag",
        "no_consumer_lag_data": "No consumer lag data",
        "latency_header": "â±ï¸ Network Request Latency",
        "latency_chart_title": "Request Latency (Last Hour)",
        "current_latency_metric": "Current Latency",
        "no_latency_data": "No latency data available",
        "resources_cpu_header": "ğŸ’» CPU Usage",
        "resources_cpu_chart_title": "CPU Usage (Last Hour)",
        "no_cpu_data": "No CPU usage data available",
        "resources_memory_header": "ğŸ§  Memory Usage",
        "resources_memory_chart_title": "Memory Usage (Last Hour)",
        "no_memory_data": "No memory usage data available",
        "error_rates_header": "ğŸ”¥ Error Rates",
        "error_rates_chart_title": "Application Error Rate (Last 5m)",
        "no_error_data": "No error rate data available",

        # 4_Demo_Scenarios.py
        "scenarios_title": "ğŸ® Interactive Demo Scenarios",
        "scenarios_header": "Hands-on Learning Scenarios",
        "choose_scenario": "ğŸ¯ Choose Learning Scenario",
        "select_scenario": "Select a scenario:",
        "scenario_a_title": "Scenario A: Normal Operations Monitoring",
        "scenario_b_title": "Scenario B: Performance Troubleshooting",
        "scenario_c_title": "Scenario C: Failure Recovery",
        "scenario_d_title": "Scenario D: Scaling Operations",

        "scenario_a_header": "ğŸ¯ Scenario A: Normal Operations Monitoring",
        "scenario_a_objectives_header": "Learning Objectives",
        "scenario_a_objectives_content": """
1.  ğŸš€ Launch complete observability environment
2.  ğŸ“Š Understand baseline performance metrics
3.  ğŸ” Explore Grafana dashboards
4.  ğŸš¨ Set up basic alerting rules
""",
        "scenario_a_guide_header": "ğŸ“ Step-by-Step Guide",
        "scenario_a_step1_header": "Step 1: Environment Launch",
        "scenario_a_step1_goal": "**Goal**: Start all services and verify status",
        "scenario_a_step1_button": "ğŸš€ Start Environment",
        "scenario_a_step1_spinner": "Starting services...",
        "scenario_a_step1_success": "âœ… Environment started successfully!",
        "scenario_a_step1_error": "âŒ Failed to start:",
        "scenario_a_step1_manual": """
# Manual start commands
cd kakfa-o11y
./start.sh

# Or use Docker Compose directly
docker compose up -d
""",
        "scenario_a_step2_header": "Step 2: Service Status Check",
        "scenario_a_step2_goal": "**Goal**: Confirm all services are running properly",
        "scenario_a_step2_button": "ğŸ” Check Service Status",
        "scenario_a_step2_spinner": "Checking services...",
        "scenario_a_step2_service_col": "Service",
        "scenario_a_step2_status_col": "Status",
        "scenario_a_step3_header": "Step 3: Observe Producer/Consumer Activity",
        "scenario_a_step3_goal": "**Goal**: Watch message flow and baseline metrics",
        "scenario_a_step3_producer_button": "ğŸ“¤ Watch Python Producer",
        "scenario_a_step3_consumer_button": "ğŸ“¥ Watch Go Consumer",
        "scenario_a_step3_spinner": "Fetching logs...",
        "scenario_a_step4_header": "Step 4: Grafana Dashboard Exploration",
        "scenario_a_step4_goal": "**Goal**: Familiarize with monitoring interface",
        "scenario_a_step4_access_header": "ğŸ“Š Access Information",
        "scenario_a_step4_url": "- URL: http://localhost:3000",
        "scenario_a_step4_user": "- Username: admin",
        "scenario_a_step4_pass": "- Password: admin",
        "scenario_a_step4_button": "ğŸ”— Open Grafana",
        "scenario_a_step4_link": "[Click to Open Grafana](http://localhost:3000)",
        "scenario_a_step4_info": "ğŸ’¡ Please change default password after first login",

        "scenario_b_header": "ğŸ”§ Scenario B: Performance Troubleshooting",
        "scenario_b_objectives_header": "Learning Objectives",
        "scenario_b_objectives_content": """
1.  ğŸ› Introduce artificial latency and errors
2.  ğŸ“‰ Observe metric degradation
3.  ğŸ” Use logs to identify root cause
4.  ğŸ”— Trace request flow
""",
        "scenario_b_step1_header": "Step 1: Simulate Network Issues",
        "scenario_b_step1_goal": "**Goal**: Pause Kafka service and observe impact",
        "scenario_b_step1_pause_button": "â¸ï¸ Pause Kafka",
        "scenario_b_step1_pause_spinner": "Pausing Kafka service...",
        "scenario_b_step1_pause_success": "âœ… Kafka paused.",
        "scenario_b_step1_pause_error": "âŒ Failed to pause Kafka:",
        "scenario_b_step1_unpause_button": "â¯ï¸ Unpause Kafka",
        "scenario_b_step1_unpause_spinner": "Unpausing Kafka service...",
        "scenario_b_step1_unpause_success": "âœ… Kafka unpaused.",
        "scenario_b_step1_unpause_error": "âŒ Failed to unpause Kafka:",

        "scenario_c_header": "ğŸ’¥ Scenario C: Failure Recovery",
        "scenario_c_objectives_header": "Learning Objectives",
        "scenario_c_objectives_content": """
1.  ğŸ”¥ Simulate a broker failure
2.  ğŸ”„ Monitor cluster recovery
3.  âš–ï¸ Analyze consumer rebalancing
4.  âœ… Validate data consistency
""",
        "scenario_c_step1_header": "Step 1: Simulate Broker Failure",
        "scenario_c_step1_goal": "**Goal**: Stop the Kafka container and observe recovery",
        "scenario_c_step1_stop_button": "ğŸ”¥ Stop Kafka Broker",
        "scenario_c_step1_stop_spinner": "Stopping Kafka broker...",
        "scenario_c_step1_stop_success": "âœ… Kafka broker stopped.",
        "scenario_c_step1_stop_error": "âŒ Failed to stop Kafka:",
        "scenario_c_step1_start_button": "â™»ï¸ Restart Kafka Broker",
        "scenario_c_step1_start_spinner": "Restarting Kafka broker...",
        "scenario_c_step1_start_success": "âœ… Kafka broker restarted.",
        "scenario_c_step1_start_error": "âŒ Failed to restart Kafka:",

        "scenario_d_header": "ğŸ“ˆ Scenario D: Scaling Operations",
        "scenario_d_objectives_header": "Learning Objectives",
        "scenario_d_objectives_content": """
1.  ğŸ“ˆ Increase message load
2.  ğŸ“Š Monitor resource utilization
3.  âš™ï¸ Scale consumers dynamically
4.  ğŸš€ Observe performance improvements
""",
        "scenario_d_step1_header": "Step 1: Scale Consumers",
        "scenario_d_step1_goal": "**Goal**: Increase the number of consumer instances",
        "scenario_d_step1_label": "Number of Python Consumers",
        "scenario_d_step1_button": "âš™ï¸ Scale Consumers",
        "scenario_d_step1_spinner": "Scaling consumers...",
        "scenario_d_step1_success": "âœ… Consumers scaled successfully!",
        "scenario_d_step1_error": "âŒ Failed to scale consumers:",

        # 6_API_Reference.py
        "api_ref_title": "ğŸ”— API & Endpoint Reference",
        "api_ref_header": "This section provides the main service endpoints used in this demo.",
        "api_ref_obs_header": "ğŸ‘ï¸ Observability Services",
        "api_ref_obs_content": """
- **Grafana**: [http://localhost:3000](http://localhost:3000)
  - *User*: `admin`
  - *Password*: `grafana`
- **Prometheus**: [http://localhost:9090](http://localhost:9090)
- **Loki**: (Accessed via Grafana)
- **Tempo**: (Accessed via Grafana)
""",
        "api_ref_kafka_header": "ğŸ“¨ Kafka Services",
        "api_ref_kafka_content": """
- **Kafka Broker**: `kafka:9092` (internal to Docker network)
- **Zookeeper**: `zookeeper:2181` (internal to Docker network)
- **JMX Exporter (Kafka)**: `jmx-exporter-kafka:5556`
- **JMX Exporter (Zookeeper)**: `jmx-exporter-zookeeper:5557`
""",
        "api_ref_otel_header": "ğŸ”­ OpenTelemetry Collector",
        "api_ref_otel_content": """
- **gRPC Endpoint**: `otel-collector:4317`
- **HTTP Endpoint**: `otel-collector:4318`
""",

        # 7_Troubleshooting.py
        "troubleshooting_title": "ğŸ› ï¸ Troubleshooting",
        "troubleshooting_header": "ğŸš« Common Issues & Solutions",
        "issue_services_fail_header": "Services fail to start or keep restarting",
        "issue_services_fail_content": """
- **Check Docker Resources**: Ensure Docker Desktop has enough memory allocated (>8GB recommended).
- **Check Port Conflicts**: Make sure ports `3000`, `8501`, `9090`, etc., are not in use.
- **View Logs**: `docker compose logs [service_name]`
- **Prune Old Containers**: `docker compose down -v`
""",
        "issue_no_data_header": "No data in Grafana",
        "issue_no_data_content": """
- **Check Prometheus**: Browse to `http://localhost:9090` and check if targets are up.
- **Check Datasource**: Verify the Prometheus datasource is correctly configured in Grafana (`http://prometheus:9090`).
- **Wait for Data**: Allow a few minutes for applications to generate telemetry.
- **Check Collector Logs**: `docker compose logs otel-collector`
""",
        "issue_streamlit_error_header": "Errors on Streamlit page",
        "issue_streamlit_error_content": """
- **Check Streamlit Logs**: `docker compose logs streamlit-app`
- **Check Dependencies**: Ensure packages in `requirements.txt` were installed successfully.
- **Rebuild the Image**: `docker compose up -d --build streamlit-app`
""",
        "troubleshooting_info": "If problems persist, please open an issue on the project's GitHub repository.",
    },
    "ç¹é«”ä¸­æ–‡": {
        # app.py
        "app_title": "Kafka APM èˆ‡å¯è§€æ¸¬æ€§ç¤ºç¯„",
        "welcome_header": "æ­¡è¿ä¾†åˆ° Kafka å¯è§€æ¸¬æ€§ç¤ºç¯„ï¼",
        "welcome_intro": """
æœ¬å°ˆæ¡ˆæ—¨åœ¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ **OpenTelemetry** èˆ‡ **Grafana** ç”Ÿæ…‹ç³»ï¼Œç‚ºåŸºæ–¼ Kafka çš„æ‡‰ç”¨ç¨‹å¼å»ºæ§‹ä¸€å€‹å…¨é¢çš„ç›£æ§ã€æ—¥èªŒèˆ‡è¿½è¹¤è§£æ±ºæ–¹æ¡ˆã€‚

ğŸ‘ˆ **è«‹å¾å´é‚Šæ¬„é¸æ“‡ä¸€å€‹é é¢é–‹å§‹ã€‚**
""",
        "project_goals_header": "ğŸš€ å°ˆæ¡ˆç›®æ¨™",
        "project_goals_content": """
- **ç«¯åˆ°ç«¯å¯è§€æ¸¬æ€§**: å¯¦ç¾å¾ç”Ÿç”¢è€…åˆ°æ¶ˆè²»è€…çš„æŒ‡æ¨™ã€æ—¥èªŒèˆ‡è¿½è¹¤å…¨è¦†è“‹ã€‚
- **å¤šèªè¨€æ”¯æ´**: å±•ç¤ºå¦‚ä½•ç›£æ§ä»¥ä¸åŒèªè¨€ï¼ˆJavaã€Pythonã€Goï¼‰ç·¨å¯«çš„æ‡‰ç”¨ç¨‹å¼ã€‚
- **äº’å‹•å¼é«”é©—**: æä¾›ä¸€å€‹ Streamlit æ‡‰ç”¨ç¨‹å¼ï¼Œç”¨æ–¼å³æ™‚ç›£æ§ã€æƒ…å¢ƒæ¨¡æ“¬èˆ‡è³‡æ–™æ¢ç´¢ã€‚
- **æœ€ä½³å¯¦è¸**: æ‡‰ç”¨å¯è§€æ¸¬æ€§è¨­è¨ˆæ¨¡å¼ï¼Œå¦‚é›†ä¸­æ”¶é›†èˆ‡çµæ§‹åŒ–æ—¥èªŒã€‚
""",
        "ready_message": "å°ˆæ¡ˆå·²æº–å‚™å°±ç·’ï¼æ‰€æœ‰æœå‹™æ‡‰å·²åœ¨ Docker ä¸­é‹è¡Œã€‚",
        "before_you_begin_header": "âœ¨ é–‹å§‹ä¹‹å‰",
        "before_you_begin_content": "è«‹ç¢ºä¿æ‚¨å·²é–±è®€å°ˆæ¡ˆçš„ `README.md` ä¸¦å·²åŸ·è¡Œ `docker-compose up -d` ä¾†å•Ÿå‹•æ‰€æœ‰æœå‹™ã€‚",
        "language_select": "é¸æ“‡èªè¨€ï¼š",

        # 1_Architecture.py
        "arch_title": "ğŸ—ï¸ ç³»çµ±æ¶æ§‹",
        "arch_overview_header": "å¯è§€æ¸¬æ€§æ¶æ§‹ç¸½è¦½",
        "arch_principles_header": "ğŸ¯ é—œéµæ¶æ§‹åŸå‰‡",
        "arch_principle_central": "é›†ä¸­åŒ–æ”¶é›†",
        "arch_principle_central_desc": "ä½¿ç”¨å–®ä¸€ OpenTelemetry Collector æ”¶é›†æ‰€æœ‰é™æ¸¬è³‡æ–™",
        "arch_principle_vendor": "å» å•†ä¸­ç«‹æ€§",
        "arch_principle_vendor_desc": "åŸºæ–¼æ¨™æº–çš„ OpenTelemetry å¯¦ä½œ",
        "arch_principle_scale": "å¯æ“´å±•æ€§",
        "arch_principle_scale_desc": "å…·å‚™æŒä¹…åŒ–å„²å­˜çš„æ°´å¹³å¯æ“´å±•å…ƒä»¶",
        "arch_principle_security": "å®‰å…¨æ€§",
        "arch_principle_security_desc": "å¯é…ç½®çš„èªè­‰èˆ‡åŠ å¯†æ”¯æ´",
        "arch_principle_ha": "é«˜å¯ç”¨æ€§",
        "arch_principle_ha_desc": "å…·å‚™å¥åº·æª¢æŸ¥å’Œè‡ªå‹•å¾©åŸçš„å½ˆæ€§è¨­è¨ˆ",
        "data_flow_header": "ğŸŒŠ è³‡æ–™æµæ¶æ§‹",
        "perf_header": "âš¡ æ•ˆèƒ½ç‰¹æ€§",
        "perf_component": "å…ƒä»¶",
        "perf_throughput": "é æœŸååé‡",
        "perf_latency": "å»¶é² (P95)",
        "perf_memory": "è¨˜æ†¶é«”ä½¿ç”¨é‡",

        # 0_Tutorial_Introduction.py
        "intro_title": "æ•™å­¸ç°¡ä»‹",
        "intro_markdown": """
æœ¬äº’å‹•å¼æ•™å­¸æ—¨åœ¨å¼•å°æ‚¨å®Œæˆ **Kafka APM èˆ‡å¯è§€æ¸¬æ€§è§£æ±ºæ–¹æ¡ˆç¤ºç¯„**ã€‚
æ‚¨å°‡å­¸ç¿’ç³»çµ±æ¶æ§‹ã€æŠ€è¡“å †ç–Šï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ç›£æ§å·¥å…·ä¾†æ’è§£æ•ˆèƒ½å•é¡Œã€‚
""",
        "intro_info": "è«‹ä¾åºç€è¦½å´é‚Šæ¬„ä¸­çš„é é¢ä»¥å®Œæˆæ•™å­¸ã€‚æˆ‘å€‘å»ºè­°å¾ã€Œæ¶æ§‹ã€é é¢é–‹å§‹ã€‚",
        "intro_structure_header": "æ•™å­¸çµæ§‹",
        "intro_structure_content": """
- **æ¶æ§‹**: äº†è§£æ•´é«”ç³»çµ±è¨­è¨ˆã€‚
- **æŠ€è¡“å †ç–Š**: äº†è§£æ‰€ä½¿ç”¨çš„æŠ€è¡“ã€‚
- **å³æ™‚å„€è¡¨æ¿**: æŸ¥çœ‹ Grafana å„€è¡¨æ¿çš„å¯¦éš›é‹ä½œã€‚
- **ç¤ºç¯„æƒ…å¢ƒ**: åŸ·è¡Œäº’å‹•å¼æƒ…å¢ƒä»¥æ¨¡æ“¬çœŸå¯¦ä¸–ç•Œçš„æƒ…æ³ã€‚
- **é‚„æœ‰æ›´å¤š...**
""",
        "intro_prereq_header": "å‰ç½®éœ€æ±‚",
        "intro_prereq_content": """
- **Docker**: ç¢ºä¿ Docker èˆ‡ Docker Compose å·²å®‰è£ä¸¦æ­£åœ¨é‹è¡Œã€‚
- **å°ˆæ¡ˆæª”æ¡ˆ**: æ‚¨æ‡‰å·²å°‡å°ˆæ¡ˆæª”æ¡ˆè¤‡è£½åˆ°æœ¬æ©Ÿã€‚
- **åŸºç¤ Kafka çŸ¥è­˜**: ç†Ÿæ‚‰åŸºç¤ Kafka æ¦‚å¿µæœƒæœ‰æ‰€å¹«åŠ©ï¼Œä½†éå¿…è¦ã€‚
""",

        # 2_Tech_Stack.py
        "tech_stack_title": "ğŸ”§ æŠ€è¡“å †ç–Š",
        "tech_app_layer": "ğŸ“± æ‡‰ç”¨ç¨‹å¼ç›£æ¸¬å±¤",
        "tech_component": "å…ƒä»¶",
        "tech_technology": "æŠ€è¡“",
        "tech_purpose": "ç”¨é€”",
        "tech_features": "ä¸»è¦ç‰¹è‰²",
        "tech_java_apps": "Java æ‡‰ç”¨ç¨‹å¼",
        "tech_python_apps": "Python æ‡‰ç”¨ç¨‹å¼",
        "tech_go_apps": "Go æ‡‰ç”¨ç¨‹å¼",
        "tech_java_agent": "OpenTelemetry Java Agent",
        "tech_python_sdk": "OpenTelemetry Python SDK",
        "tech_go_sdk": "OpenTelemetry Go SDK",
        "tech_auto_instrument": "è‡ªå‹•ç›£æ¸¬",
        "tech_manual_instrument": "æ‰‹å‹•ç›£æ¸¬",
        "tech_java_features": "é›¶ç¨‹å¼ç¢¼ç›£æ¸¬ã€JVM æŒ‡æ¨™ã€Kafka å®¢æˆ¶ç«¯è¿½è¹¤",
        "tech_python_features": "è‡ªè¨‚æŒ‡æ¨™ã€çµæ§‹åŒ–æ—¥èªŒã€éåŒæ­¥æ”¯æ´",
        "tech_go_features": "é«˜æ•ˆèƒ½è¿½è¹¤ã€è‡ªè¨‚æŒ‡æ¨™ã€ä½è² æ“”",
        "tech_streaming_platform": "ğŸ“¨ è¨Šæ¯ä¸²æµå¹³å°",
        "tech_kafka": "Apache Kafka",
        "tech_zookeeper": "Zookeeper",
        "tech_jmx": "JMX Exporters",
        "tech_confluent": "Confluent Platform",
        "tech_apache_zookeeper": "Apache Zookeeper",
        "tech_prometheus_jmx": "Prometheus JMX Exporter",
        "tech_broker": "è¨Šæ¯ä»£ç†ä¼ºæœå™¨",
        "tech_coordination": "å”èª¿æœå‹™",
        "tech_metrics_collection": "æŒ‡æ¨™æ”¶é›†",
        "tech_kafka_features": "é«˜ååé‡ã€è€ä¹…æ€§ã€åŸºæ–¼åˆ†å€çš„æ“´å±•",
        "tech_zookeeper_features": "å¢é›†å”èª¿ã€é…ç½®ç®¡ç†",
        "tech_jmx_features": "ä»£ç†ä¼ºæœå™¨æŒ‡æ¨™ã€JVM çµ±è¨ˆã€ä¸»é¡ŒæŒ‡æ¨™",
        "tech_obs_infra": "ğŸ‘ï¸ å¯è§€æ¸¬æ€§åŸºç¤è¨­æ–½",
        "tech_otel_collector": "OpenTelemetry Collector",
        "tech_prometheus": "Prometheus",
        "tech_loki": "Loki",
        "tech_tempo": "Tempo",
        "tech_grafana": "Grafana",
        "tech_otel_contrib": "OTEL Collector Contrib",
        "tech_prometheus_tsdb": "Prometheus TSDB",
        "tech_grafana_loki": "Grafana Loki",
        "tech_grafana_tempo": "Grafana Tempo",
        "tech_grafana_viz": "Grafana",
        "tech_pipeline": "é™æ¸¬ç®¡é“",
        "tech_metrics_storage": "æŒ‡æ¨™å„²å­˜",
        "tech_log_aggregation": "æ—¥èªŒèšåˆ",
        "tech_trace_storage": "è¿½è¹¤å„²å­˜",
        "tech_visualization": "è¦–è¦ºåŒ–",
        "tech_otel_features": "è³‡æ–™æ”¶é›†ã€è™•ç†ã€è·¯ç”±ã€åŒ¯å‡º",
        "tech_prometheus_features": "æ™‚é–“åºåˆ—è³‡æ–™åº«ã€PromQL æŸ¥è©¢ã€å‘Šè­¦",
        "tech_loki_features": "æ—¥èªŒç´¢å¼•ã€LogQL æŸ¥è©¢ã€æ¨™ç±¤çµ„ç¹”",
        "tech_tempo_features": "åˆ†æ•£å¼è¿½è¹¤ã€è¿½è¹¤é—œè¯ã€å–æ¨£",
        "tech_grafana_features": "å„€è¡¨æ¿ã€å‘Šè­¦ã€è³‡æ–™æ¢ç´¢",

        # 4_Demo_Scenarios.py
        "scenarios_title": "ğŸ® äº’å‹•å¼ç¤ºç¯„æƒ…å¢ƒ",
        "scenarios_header": "å‹•æ‰‹å­¸ç¿’æƒ…å¢ƒ",
        "choose_scenario": "ğŸ¯ é¸æ“‡å­¸ç¿’æƒ…å¢ƒ",
        "select_scenario": "é¸æ“‡æƒ…å¢ƒ:",
        "scenario_a_title": "æƒ…å¢ƒ A: æ­£å¸¸ç‡Ÿé‹ç›£æ§",
        "scenario_b_title": "æƒ…å¢ƒ B: æ•ˆèƒ½æ•…éšœæ’é™¤",
        "scenario_c_title": "æƒ…å¢ƒ C: æ•…éšœå¾©åŸ",
        "scenario_d_title": "æƒ…å¢ƒ D: æ“´å±•ç‡Ÿé‹",

        "scenario_a_header": "ğŸ¯ æƒ…å¢ƒ A: æ­£å¸¸ç‡Ÿé‹ç›£æ§",
        "scenario_a_objectives_header": "å­¸ç¿’ç›®æ¨™",
        "scenario_a_objectives_content": """
1.  ğŸš€ å•Ÿå‹•å®Œæ•´çš„å¯è§€æ¸¬æ€§ç’°å¢ƒ
2.  ğŸ“Š äº†è§£åŸºæº–æ•ˆèƒ½æŒ‡æ¨™
3.  ğŸ” æ¢ç´¢ Grafana å„€è¡¨æ¿
4.  ğŸš¨ è¨­å®šåŸºæœ¬å‘Šè­¦è¦å‰‡
""",
        "scenario_a_guide_header": "ğŸ“ é€æ­¥æŒ‡å°",
        "scenario_a_step1_header": "æ­¥é©Ÿ 1: ç’°å¢ƒå•Ÿå‹•",
        "scenario_a_step1_goal": "**ç›®æ¨™**: å•Ÿå‹•æ‰€æœ‰æœå‹™ä¸¦é©—è­‰ç‹€æ…‹",
        "scenario_a_step1_button": "ğŸš€ å•Ÿå‹•ç’°å¢ƒ",
        "scenario_a_step1_spinner": "æ­£åœ¨å•Ÿå‹•æœå‹™...",
        "scenario_a_step1_success": "âœ… ç’°å¢ƒå•Ÿå‹•æˆåŠŸ!",
        "scenario_a_step1_error": "âŒ å•Ÿå‹•å¤±æ•—:",
        "scenario_a_step1_manual": """
# æ‰‹å‹•å•Ÿå‹•æŒ‡ä»¤
cd kakfa-o11y
./start.sh

# æˆ–ä½¿ç”¨ Docker Compose
docker compose up -d
""",
        "scenario_a_step2_header": "æ­¥é©Ÿ 2: æœå‹™ç‹€æ…‹æª¢æŸ¥",
        "scenario_a_step2_goal": "**ç›®æ¨™**: ç¢ºèªæ‰€æœ‰æœå‹™æ­£å¸¸é‹è¡Œ",
        "scenario_a_step2_button": "ğŸ” æª¢æŸ¥æœå‹™ç‹€æ…‹",
        "scenario_a_step2_spinner": "æª¢æŸ¥æœå‹™ä¸­...",
        "scenario_a_step2_service_col": "æœå‹™",
        "scenario_a_step2_status_col": "ç‹€æ…‹",
        "scenario_a_step3_header": "æ­¥é©Ÿ 3: è§€å¯Ÿç”Ÿç”¢è€…/æ¶ˆè²»è€…æ´»å‹•",
        "scenario_a_step3_goal": "**ç›®æ¨™**: è§€å¯Ÿè¨Šæ¯æµå’ŒåŸºæº–æŒ‡æ¨™",
        "scenario_a_step3_producer_button": "ğŸ“¤ è§€å¯Ÿ Python Producer",
        "scenario_a_step3_consumer_button": "ğŸ“¥ è§€å¯Ÿ Go Consumer",
        "scenario_a_step3_spinner": "ç²å–æ—¥èªŒ...",
        "scenario_a_step4_header": "æ­¥é©Ÿ 4: Grafana å„€è¡¨æ¿æ¢ç´¢",
        "scenario_a_step4_goal": "**ç›®æ¨™**: ç†Ÿæ‚‰ç›£æ§ä»‹é¢",
        "scenario_a_step4_access_header": "ğŸ“Š å­˜å–è³‡è¨Š:",
        "scenario_a_step4_url": "- URL: http://localhost:3000",
        "scenario_a_step4_user": "- å¸³è™Ÿ: admin",
        "scenario_a_step4_pass": "- å¯†ç¢¼: admin",
        "scenario_a_step4_button": "ğŸ”— é–‹å•Ÿ Grafana",
        "scenario_a_step4_link": "[é»æ“Šé–‹å•Ÿ Grafana](http://localhost:3000)",
        "scenario_a_step4_info": "ğŸ’¡ é¦–æ¬¡ç™»å…¥å¾Œè«‹æ›´æ”¹é è¨­å¯†ç¢¼",

        "scenario_b_header": "ğŸ”§ æƒ…å¢ƒ B: æ•ˆèƒ½æ•…éšœæ’é™¤",
        "scenario_b_objectives_header": "å­¸ç¿’ç›®æ¨™",
        "scenario_b_objectives_content": """
1.  ğŸ› å¼•å…¥äººå·¥å»¶é²å’ŒéŒ¯èª¤
2.  ğŸ“‰ è§€å¯ŸæŒ‡æ¨™è¡°é€€
3.  ğŸ” ä½¿ç”¨æ—¥èªŒè­˜åˆ¥æ ¹æœ¬åŸå› 
4.  ğŸ”— è¿½è¹¤è«‹æ±‚æµç¨‹
""",
        "scenario_b_step1_header": "æ­¥é©Ÿ 1: æ¨¡æ“¬ç¶²è·¯å•é¡Œ",
        "scenario_b_step1_goal": "**ç›®æ¨™**: æš«åœ Kafka æœå‹™ä¸¦è§€å¯Ÿå½±éŸ¿",
        "scenario_b_step1_pause_button": "â¸ï¸ æš«åœ Kafka",
        "scenario_b_step1_pause_spinner": "æš«åœ Kafka æœå‹™...",
        "scenario_b_step1_pause_success": "âœ… Kafka å·²æš«åœã€‚",
        "scenario_b_step1_pause_error": "âŒ æš«åœ Kafka å¤±æ•—:",
        "scenario_b_step1_unpause_button": "â¯ï¸ æ¢å¾© Kafka",
        "scenario_b_step1_unpause_spinner": "æ¢å¾© Kafka æœå‹™...",
        "scenario_b_step1_unpause_success": "âœ… Kafka å·²æ¢å¾©ã€‚",
        "scenario_b_step1_unpause_error": "âŒ æ¢å¾© Kafka å¤±æ•—:",

        "scenario_c_header": "ğŸ’¥ æƒ…å¢ƒ C: æ•…éšœå¾©åŸ",
        "scenario_c_objectives_header": "å­¸ç¿’ç›®æ¨™",
        "scenario_c_objectives_content": """
1.  ğŸ”¥ æ¨¡æ“¬ä»£ç†ä¼ºæœå™¨æ•…éšœ
2.  ğŸ”„ ç›£æ§å¢é›†å¾©åŸ
3.  âš–ï¸ åˆ†ææ¶ˆè²»è€…é‡æ–°å¹³è¡¡
4.  âœ… é©—è­‰è³‡æ–™ä¸€è‡´æ€§
""",
        "scenario_c_step1_header": "æ­¥é©Ÿ 1: æ¨¡æ“¬ä»£ç†ä¼ºæœå™¨æ•…éšœ",
        "scenario_c_step1_goal": "**ç›®æ¨™**: åœæ­¢ Kafka å®¹å™¨ä¸¦è§€å¯Ÿå¾©åŸæƒ…æ³",
        "scenario_c_step1_stop_button": "ğŸ”¥ åœæ­¢ Kafka ä»£ç†ä¼ºæœå™¨",
        "scenario_c_step1_stop_spinner": "æ­£åœ¨åœæ­¢ Kafka ä»£ç†ä¼ºæœå™¨...",
        "scenario_c_step1_stop_success": "âœ… Kafka ä»£ç†ä¼ºæœå™¨å·²åœæ­¢ã€‚",
        "scenario_c_step1_stop_error": "âŒ åœæ­¢ Kafka å¤±æ•—:",
        "scenario_c_step1_start_button": "â™»ï¸ é‡å•Ÿ Kafka ä»£ç†ä¼ºæœå™¨",
        "scenario_c_step1_start_spinner": "æ­£åœ¨é‡å•Ÿ Kafka ä»£ç†ä¼ºæœå™¨...",
        "scenario_c_step1_start_success": "âœ… Kafka ä»£ç†ä¼ºæœå™¨å·²é‡å•Ÿã€‚",
        "scenario_c_step1_start_error": "âŒ é‡å•Ÿ Kafka å¤±æ•—:",

        "scenario_d_header": "ğŸ“ˆ æƒ…å¢ƒ D: æ“´å±•ç‡Ÿé‹",
        "scenario_d_objectives_header": "å­¸ç¿’ç›®æ¨™",
        "scenario_d_objectives_content": """
1.  ğŸ“ˆ å¢åŠ è¨Šæ¯è² è¼‰
2.  ğŸ“Š ç›£æ§è³‡æºä½¿ç”¨ç‡
3.  âš™ï¸ å‹•æ…‹æ“´å±•æ¶ˆè²»è€…
4.  ğŸš€ è§€å¯Ÿæ•ˆèƒ½æ”¹å–„
""",
        "scenario_d_step1_header": "æ­¥é©Ÿ 1: æ“´å±•æ¶ˆè²»è€…",
        "scenario_d_step1_goal": "**ç›®æ¨™**: å¢åŠ æ¶ˆè²»è€…å¯¦ä¾‹çš„æ•¸é‡",
        "scenario_d_step1_label": "Python æ¶ˆè²»è€…æ•¸é‡",
        "scenario_d_step1_button": "âš™ï¸ æ“´å±•æ¶ˆè²»è€…",
        "scenario_d_step1_spinner": "æ­£åœ¨æ“´å±•æ¶ˆè²»è€…...",
        "scenario_d_step1_success": "âœ… æ¶ˆè²»è€…æ“´å±•æˆåŠŸ!",
        "scenario_d_step1_error": "âŒ æ¶ˆè²»è€…æ“´å±•å¤±æ•—:",

        # 6_API_Reference.py
        "api_ref_title": "ğŸ”— API & ç«¯é»åƒè€ƒ",
        "api_ref_header": "æ­¤è™•æä¾›åœ¨æ­¤ç¤ºç¯„ä¸­ä½¿ç”¨çš„ä¸»è¦æœå‹™ç«¯é»ã€‚",
        "api_ref_obs_header": "ğŸ‘ï¸ å¯è§€æ¸¬æ€§æœå‹™",
        "api_ref_obs_content": """
- **Grafana**: [http://localhost:3000](http://localhost:3000)
  - *ä½¿ç”¨è€…*: `admin`
  - *å¯†ç¢¼*: `grafana`
- **Prometheus**: [http://localhost:9090](http://localhost:9090)
- **Loki**: (é€é Grafana å­˜å–)
- **Tempo**: (é€é Grafana å­˜å–)
""",
        "api_ref_kafka_header": "ğŸ“¨ Kafka æœå‹™",
        "api_ref_kafka_content": """
- **Kafka ä»£ç†ä¼ºæœå™¨**: `kafka:9092` (Docker ç¶²è·¯å…§éƒ¨)
- **Zookeeper**: `zookeeper:2181` (Docker ç¶²è·¯å…§éƒ¨)
- **JMX Exporter (Kafka)**: `jmx-exporter-kafka:5556`
- **JMX Exporter (Zookeeper)**: `jmx-exporter-zookeeper:5557`
""",
        "api_ref_otel_header": "ğŸ”­ OpenTelemetry Collector",
        "api_ref_otel_content": """
- **gRPC ç«¯é»**: `otel-collector:4317`
- **HTTP ç«¯é»**: `otel-collector:4318`
""",

        # 7_Troubleshooting.py
        "troubleshooting_title": "ğŸ› ï¸ å•é¡Œæ’è§£",
        "troubleshooting_header": "ğŸš« å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ",
        "issue_services_fail_header": "æœå‹™ç„¡æ³•å•Ÿå‹•æˆ–æŒçºŒé‡å•Ÿ",
        "issue_services_fail_content": """
- **æª¢æŸ¥ Docker è³‡æº**: ç¢ºä¿ Docker Desktop åˆ†é…äº†è¶³å¤ çš„è¨˜æ†¶é«”ï¼ˆå»ºè­° >8GBï¼‰ã€‚
- **æª¢æŸ¥åŸ è™Ÿè¡çª**: ç¢ºä¿ `3000`, `8501`, `9090` ç­‰åŸ è™Ÿæœªè¢«å…¶ä»–æ‡‰ç”¨ç¨‹å¼å ç”¨ã€‚
- **æŸ¥çœ‹æ—¥èªŒ**: `docker compose logs [service_name]`
- **æ¸…é™¤èˆŠå®¹å™¨**: `docker compose down -v`
""",
        "issue_no_data_header": "Grafana ä¸­æ²’æœ‰è³‡æ–™",
        "issue_no_data_content": """
- **æª¢æŸ¥ Prometheus**: ç€è¦½ `http://localhost:9090` ç¢ºèªç›®æ¨™æ˜¯å¦æ­£å¸¸ã€‚
- **æª¢æŸ¥è³‡æ–™ä¾†æº**: åœ¨ Grafana ä¸­ç¢ºèª Prometheus è³‡æ–™ä¾†æºè¨­å®šæ­£ç¢º (`http://prometheus:9090`)ã€‚
- **ç­‰å¾…è³‡æ–™ç”¢ç”Ÿ**: å•Ÿå‹•å¾Œéœ€è¦å¹¾åˆ†é˜æ™‚é–“è®“æ‡‰ç”¨ç¨‹å¼ç”¢ç”Ÿé™æ¸¬è³‡æ–™ã€‚
- **æª¢æŸ¥ Collector æ—¥èªŒ**: `docker compose logs otel-collector`
""",
        "issue_streamlit_error_header": "Streamlit é é¢éŒ¯èª¤",
        "issue_streamlit_error_content": """
- **æŸ¥çœ‹ Streamlit æ—¥èªŒ**: `docker compose logs streamlit-app`
- **æª¢æŸ¥ç›¸ä¾å¥—ä»¶**: `requirements.txt` ä¸­çš„å¥—ä»¶æ˜¯å¦å®‰è£æˆåŠŸã€‚
- **é‡å»ºç«‹æ˜ åƒæª”**: `docker compose up -d --build streamlit-app`
""",
        "troubleshooting_info": "å¦‚æœå•é¡Œä»ç„¶å­˜åœ¨ï¼Œè«‹åœ¨å°ˆæ¡ˆçš„ GitHub Issues ä¸­æå‡ºã€‚",
    }
} 