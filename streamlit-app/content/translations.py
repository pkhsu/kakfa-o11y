def get_text(language, key):
    # Fallback to English if the key is not found in the selected language
    return TRANSLATIONS.get(language, {}).get(key) or TRANSLATIONS.get("English", {}).get(key, f"Missing translation for {key}")

TRANSLATIONS = {
    "English": {
        # app.py
        "app_title": "Kafka APM & Observability Demo",
        "welcome_header": "Welcome to the Kafka Observability Demo!",
        "welcome_intro": """
This project is designed to showcase how to build a comprehensive monitoring, logging, and tracing solution for Kafka-based applications using the **OpenTelemetry** and **Grafana** ecosystems.

üëà **Select a page from the sidebar to get started.**
""",
        "project_goals_header": "üöÄ Project Goals",
        "project_goals_content": """
- **End-to-End Observability**: Achieve full coverage of metrics, logs, and traces from producer to consumer.
- **Polyglot Support**: Demonstrate how to monitor applications written in different languages (Java, Python, Go).
- **Interactive Experience**: Provide a Streamlit application for live monitoring, scenario simulation, and data exploration.
- **Best Practices**: Apply observability design patterns like centralized collection and structured logging.
""",
        "ready_message": "The project is ready to go! All services should be running in Docker.",
        "before_you_begin_header": "‚ú® Before You Begin",
        "before_you_begin_content": "Make sure you have read the project's `README.md` and have run `docker-compose up -d` to start all services.",
        "language_select": "Choose a language:",

        # 1_Architecture.py
        "arch_title": "üèóÔ∏è System Architecture",
        "arch_overview_header": "Observability Architecture Overview",
        "arch_principles_header": "üéØ Key Architecture Principles",
        "arch_principle_central": "Centralized Collection",
        "arch_principle_central_desc": "Single OpenTelemetry Collector for all telemetry data",
        "arch_principle_vendor": "Vendor Neutrality",
        "arch_principle_vendor_desc": "Standards-based OpenTelemetry implementation",
        "arch_principle_scale": "Scalability",
        "arch_principle_scale_desc": "Horizontally scalable components with persistent storage",
        "arch_principle_security": "Security",
        "arch_principle_security_desc": "Configurable authentication and encryption support",
        "arch_principle_ha": "High Availability",
        "arch_principle_ha_desc": "Resilient design with health checks and automatic recovery",
        "data_flow_header": "üåä Data Flow Architecture",
        "perf_header": "‚ö° Performance Characteristics",
        "perf_component": "Component",
        "perf_throughput": "Expected Throughput",
        "perf_latency": "Latency (P95)",
        "perf_memory": "Memory Usage",

        # 0_Tutorial_Introduction.py
        "intro_title": "Tutorial Introduction",
        "intro_markdown": """
This interactive tutorial is designed to guide you through the **Kafka APM & Observability Solution Demo**. 
You will learn about the system architecture, technology stack, and how to use the monitoring tools to troubleshoot performance issues.
""",
        "intro_info": "Please proceed through the pages in the sidebar to complete the tutorial. We recommend starting with the 'Architecture' page.",
        "intro_structure_header": "Tutorial Structure",
        "intro_structure_content": """
- **Architecture**: Understand the overall system design.
- **Tech Stack**: Learn about the technologies used.
- **Live Dashboard**: See the Grafana dashboard in action.
- **Demo Scenarios**: Run interactive scenarios to simulate real-world situations.
- **And more...**
""",
        "intro_prereq_header": "Prerequisites",
        "intro_prereq_content": """
- **Docker**: Ensure Docker and Docker Compose are installed and running.
- **Project Files**: You should have the project files cloned to your local machine.
- **Basic Kafka Knowledge**: Familiarity with basic Kafka concepts is helpful but not required.
""",

        # 2_Tech_Stack.py
        "tech_stack_title": "üîß Technology Stack",
        "tech_app_layer": "üì± Application Instrumentation Layer",
        "tech_component": "Component",
        "tech_technology": "Technology",
        "tech_purpose": "Purpose",
        "tech_features": "Key Features",
        "tech_java_apps": "Java Apps",
        "tech_python_apps": "Python Apps",
        "tech_go_apps": "Go Apps",
        "tech_java_agent": "OpenTelemetry Java Agent",
        "tech_python_sdk": "OpenTelemetry Python SDK",
        "tech_go_sdk": "OpenTelemetry Go SDK",
        "tech_auto_instrument": "Auto-instrumentation",
        "tech_manual_instrument": "Manual instrumentation",
        "tech_java_features": "Zero-code instrumentation, JVM metrics, Kafka client tracing",
        "tech_python_features": "Custom metrics, structured logging, async support",
        "tech_go_features": "High-performance tracing, custom metrics, low overhead",
        "tech_streaming_platform": "üì® Message Streaming Platform",
        "tech_kafka": "Apache Kafka",
        "tech_zookeeper": "Zookeeper",
        "tech_jmx": "JMX Exporters",
        "tech_confluent": "Confluent Platform",
        "tech_apache_zookeeper": "Apache Zookeeper",
        "tech_prometheus_jmx": "Prometheus JMX Exporter",
        "tech_broker": "Message Broker",
        "tech_coordination": "Coordination Service",
        "tech_metrics_collection": "Metrics Collection",
        "tech_kafka_features": "High throughput, durability, partition-based scaling",
        "tech_zookeeper_features": "Cluster coordination, configuration management",
        "tech_jmx_features": "Broker metrics, JVM stats, topic metrics",
        "tech_obs_infra": "üëÅÔ∏è Observability Infrastructure",
        "tech_otel_collector": "OpenTelemetry Collector",
        "tech_prometheus": "Prometheus",
        "tech_loki": "Loki",
        "tech_tempo": "Tempo",
        "tech_grafana": "Grafana",
        "tech_otel_contrib": "OTEL Collector Contrib",
        "tech_prometheus_tsdb": "Prometheus TSDB",
        "tech_grafana_loki": "Grafana Loki",
        "tech_grafana_tempo": "Grafana Tempo",
        "tech_grafana_viz": "Grafana",
        "tech_pipeline": "Telemetry Pipeline",
        "tech_metrics_storage": "Metrics Storage",
        "tech_log_aggregation": "Log Aggregation",
        "tech_trace_storage": "Trace Storage",
        "tech_visualization": "Visualization",
        "tech_otel_features": "Data collection, processing, routing, export",
        "tech_prometheus_features": "Time-series database, PromQL queries, alerting",
        "tech_loki_features": "Log indexing, LogQL queries, label-based organization",
        "tech_tempo_features": "Distributed tracing, trace correlation, sampling",
        "tech_grafana_features": "Dashboards, alerting, data exploration",

        # 3_Live_Dashboard.py
        "dashboard_title": "üìä Live System Dashboard",
        "dashboard_header": "Real-time Monitoring & Metrics",
        "auto_refresh": "Auto-refresh (30s)",
        "service_status": "üö• Service Status",
        "prometheus_status": "Prometheus",
        "grafana_status": "Grafana",
        "otel_collector_status": "OTel Collector",
        "kafka_status": "Kafka",
        "running": "Running",
        "down": "Down",
        "key_metrics": "üìà Key Metrics",
        "throughput_tab": "Message Throughput",
        "latency_tab": "Latency Metrics",
        "resources_tab": "System Resources",
        "errors_tab": "Error Rates",
        "producer_throughput_header": "üì§ Producer Throughput",
        "python_producer_service": "Python Producer",
        "go_producer_service": "Go Producer",
        "current_throughput_title": "Current Throughput",
        "no_throughput_data": "No throughput data available",
        "no_producer_metrics": "No producer metrics found",
        "consumer_lag_header": "üì• Consumer Lag",
        "consumer_lag_title": "Consumer Lag",
        "no_consumer_lag_data": "No consumer lag data",
        "latency_header": "‚è±Ô∏è Network Request Latency",
        "latency_chart_title": "Request Latency (Last Hour)",
        "current_latency_metric": "Current Latency",
        "no_latency_data": "No latency data available",
        "resources_cpu_header": "üíª CPU Usage",
        "resources_cpu_chart_title": "CPU Usage (Last Hour)",
        "no_cpu_data": "No CPU usage data available",
        "resources_memory_header": "üß† Memory Usage",
        "resources_memory_chart_title": "Memory Usage (Last Hour)",
        "no_memory_data": "No memory usage data available",
        "error_rates_header": "üî• Error Rates",
        "error_rates_chart_title": "Application Error Rate (Last 5m)",
        "no_error_data": "No error rate data available",

        # 4_Demo_Scenarios.py
        "scenarios_title": "üéÆ Interactive Demo Scenarios",
        "scenarios_header": "Hands-on Learning Scenarios",
        "choose_scenario": "üéØ Choose Learning Scenario",
        "select_scenario": "Select a scenario:",
        "scenario_a_title": "Scenario A: Normal Operations Monitoring",
        "scenario_b_title": "Scenario B: Performance Troubleshooting",
        "scenario_c_title": "Scenario C: Failure Recovery",
        "scenario_d_title": "Scenario D: Scaling Operations",

        "scenario_a_header": "üéØ Scenario A: Normal Operations Monitoring",
        "scenario_a_objectives_header": "Learning Objectives",
        "scenario_a_objectives_content": """
1.  üöÄ Launch complete observability environment
2.  üìä Understand baseline performance metrics
3.  üîç Explore Grafana dashboards
4.  üö® Set up basic alerting rules
""",
        "scenario_a_guide_header": "üìù Step-by-Step Guide",
        "scenario_a_step1_header": "Step 1: Environment Launch",
        "scenario_a_step1_goal": "**Goal**: Start all services and verify status",
        "scenario_a_step1_button": "üöÄ Start Environment",
        "scenario_a_step1_spinner": "Starting services...",
        "scenario_a_step1_success": "‚úÖ Environment started successfully!",
        "scenario_a_step1_error": "‚ùå Failed to start:",
        "scenario_a_step1_manual": """
# Manual start commands
cd kakfa-o11y
./start.sh

# Or use Docker Compose directly
docker compose up -d
""",
        "scenario_a_step2_header": "Step 2: Service Status Check",
        "scenario_a_step2_goal": "**Goal**: Confirm all services are running properly",
        "scenario_a_step2_button": "üîç Check Service Status",
        "scenario_a_step2_spinner": "Checking services...",
        "scenario_a_step2_service_col": "Service",
        "scenario_a_step2_status_col": "Status",
        "scenario_a_step3_header": "Step 3: Observe Producer/Consumer Activity",
        "scenario_a_step3_goal": "**Goal**: Watch message flow and baseline metrics",
        "scenario_a_step3_producer_button": "üì§ Watch Python Producer",
        "scenario_a_step3_consumer_button": "üì• Watch Go Consumer",
        "scenario_a_step3_spinner": "Fetching logs...",
        "scenario_a_step4_header": "Step 4: Grafana Dashboard Exploration",
        "scenario_a_step4_goal": "**Goal**: Familiarize with monitoring interface",
        "scenario_a_step4_access_header": "üìä Access Information",
        "scenario_a_step4_url": "- URL: http://localhost:3000",
        "scenario_a_step4_user": "- Username: admin",
        "scenario_a_step4_pass": "- Password: admin",
        "scenario_a_step4_button": "üîó Open Grafana",
        "scenario_a_step4_link": "[Click to Open Grafana](http://localhost:3000)",
        "scenario_a_step4_info": "üí° Please change default password after first login",

        "scenario_b_header": "üîß Scenario B: Performance Troubleshooting",
        "scenario_b_objectives_header": "Learning Objectives",
        "scenario_b_objectives_content": """
1.  üêõ Introduce artificial latency and errors
2.  üìâ Observe metric degradation
3.  üîç Use logs to identify root cause
4.  üîó Trace request flow
""",
        "scenario_b_step1_header": "Step 1: Simulate Network Issues",
        "scenario_b_step1_goal": "**Goal**: Pause Kafka service and observe impact",
        "scenario_b_step1_pause_button": "‚è∏Ô∏è Pause Kafka",
        "scenario_b_step1_pause_spinner": "Pausing Kafka service...",
        "scenario_b_step1_pause_success": "‚úÖ Kafka paused.",
        "scenario_b_step1_pause_error": "‚ùå Failed to pause Kafka:",
        "scenario_b_step1_unpause_button": "‚èØÔ∏è Unpause Kafka",
        "scenario_b_step1_unpause_spinner": "Unpausing Kafka service...",
        "scenario_b_step1_unpause_success": "‚úÖ Kafka unpaused.",
        "scenario_b_step1_unpause_error": "‚ùå Failed to unpause Kafka:",

        "scenario_c_header": "üí• Scenario C: Failure Recovery",
        "scenario_c_objectives_header": "Learning Objectives",
        "scenario_c_objectives_content": """
1.  üî• Simulate a broker failure
2.  üîÑ Monitor cluster recovery
3.  ‚öñÔ∏è Analyze consumer rebalancing
4.  ‚úÖ Validate data consistency
""",
        "scenario_c_step1_header": "Step 1: Simulate Broker Failure",
        "scenario_c_step1_goal": "**Goal**: Stop the Kafka container and observe recovery",
        "scenario_c_step1_stop_button": "üî• Stop Kafka Broker",
        "scenario_c_step1_stop_spinner": "Stopping Kafka broker...",
        "scenario_c_step1_stop_success": "‚úÖ Kafka broker stopped.",
        "scenario_c_step1_stop_error": "‚ùå Failed to stop Kafka:",
        "scenario_c_step1_start_button": "‚ôªÔ∏è Restart Kafka Broker",
        "scenario_c_step1_start_spinner": "Restarting Kafka broker...",
        "scenario_c_step1_start_success": "‚úÖ Kafka broker restarted.",
        "scenario_c_step1_start_error": "‚ùå Failed to restart Kafka:",

        "scenario_d_header": "üìà Scenario D: Scaling Operations",
        "scenario_d_objectives_header": "Learning Objectives",
        "scenario_d_objectives_content": """
1.  üìà Increase message load
2.  üìä Monitor resource utilization
3.  ‚öôÔ∏è Scale consumers dynamically
4.  üöÄ Observe performance improvements
""",
        "scenario_d_step1_header": "Step 1: Scale Consumers",
        "scenario_d_step1_goal": "**Goal**: Increase the number of consumer instances",
        "scenario_d_step1_label": "Number of Python Consumers",
        "scenario_d_step1_button": "‚öôÔ∏è Scale Consumers",
        "scenario_d_step1_spinner": "Scaling consumers...",
        "scenario_d_step1_success": "‚úÖ Consumers scaled successfully!",
        "scenario_d_step1_error": "‚ùå Failed to scale consumers:",

        # 6_API_Reference.py
        "api_ref_title": "üîó API & Endpoint Reference",
        "api_ref_header": "This section provides the main service endpoints used in this demo.",
        "api_ref_obs_header": "üëÅÔ∏è Observability Services",
        "api_ref_obs_content": """
- **Grafana**: [http://localhost:3000](http://localhost:3000)
  - *User*: `admin`
  - *Password*: `grafana`
- **Prometheus**: [http://localhost:9090](http://localhost:9090)
- **Loki**: (Accessed via Grafana)
- **Tempo**: (Accessed via Grafana)
""",
        "api_ref_kafka_header": "üì® Kafka Services",
        "api_ref_kafka_content": """
- **Kafka Broker**: `kafka:9092` (internal to Docker network)
- **Zookeeper**: `zookeeper:2181` (internal to Docker network)
- **JMX Exporter (Kafka)**: `jmx-exporter-kafka:5556`
- **JMX Exporter (Zookeeper)**: `jmx-exporter-zookeeper:5557`
""",
        "api_ref_otel_header": "üî≠ OpenTelemetry Collector",
        "api_ref_otel_content": """
- **gRPC Endpoint**: `otel-collector:4317`
- **HTTP Endpoint**: `otel-collector:4318`
""",

        # 7_Troubleshooting.py
        "troubleshooting_title": "üõ†Ô∏è Troubleshooting",
        "troubleshooting_header": "üö´ Common Issues & Solutions",
        "issue_services_fail_header": "Services fail to start or keep restarting",
        "issue_services_fail_content": """
- **Check Docker Resources**: Ensure Docker Desktop has enough memory allocated (>8GB recommended).
- **Check Port Conflicts**: Make sure ports `3000`, `8501`, `9090`, etc., are not in use.
- **View Logs**: `docker compose logs [service_name]`
- **Prune Old Containers**: `docker compose down -v`
""",
        "issue_no_data_header": "No data in Grafana",
        "issue_no_data_content": """
- **Check Prometheus**: Browse to `http://localhost:9090` and check if targets are up.
- **Check Datasource**: Verify the Prometheus datasource is correctly configured in Grafana (`http://prometheus:9090`).
- **Wait for Data**: Allow a few minutes for applications to generate telemetry.
- **Check Collector Logs**: `docker compose logs otel-collector`
""",
        "issue_streamlit_error_header": "Errors on Streamlit page",
        "issue_streamlit_error_content": """
- **Check Streamlit Logs**: `docker compose logs streamlit-app`
- **Check Dependencies**: Ensure packages in `requirements.txt` were installed successfully.
- **Rebuild the Image**: `docker compose up -d --build streamlit-app`
""",
        "troubleshooting_info": "If problems persist, please open an issue on the project's GitHub repository.",
    },
    "ÁπÅÈ´î‰∏≠Êñá": {
        # app.py
        "app_title": "Kafka APM ËàáÂèØËßÄÊ∏¨ÊÄßÁ§∫ÁØÑ",
        "welcome_header": "Ê≠°Ëøé‰æÜÂà∞ Kafka ÂèØËßÄÊ∏¨ÊÄßÁ§∫ÁØÑÔºÅ",
        "welcome_intro": """
Êú¨Â∞àÊ°àÊó®Âú®Â±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® **OpenTelemetry** Ëàá **Grafana** ÁîüÊÖãÁ≥ªÔºåÁÇ∫Âü∫Êñº Kafka ÁöÑÊáâÁî®Á®ãÂºèÂª∫Êßã‰∏ÄÂÄãÂÖ®Èù¢ÁöÑÁõ£Êéß„ÄÅÊó•Ë™åËàáËøΩËπ§Ëß£Ê±∫ÊñπÊ°à„ÄÇ

üëà **Ë´ãÂæûÂÅ¥ÈÇäÊ¨ÑÈÅ∏Êìá‰∏ÄÂÄãÈ†ÅÈù¢ÈñãÂßã„ÄÇ**
""",
        "project_goals_header": "üöÄ Â∞àÊ°àÁõÆÊ®ô",
        "project_goals_content": """
- **Á´ØÂà∞Á´ØÂèØËßÄÊ∏¨ÊÄß**: ÂØ¶ÁèæÂæûÁîüÁî¢ËÄÖÂà∞Ê∂àË≤ªËÄÖÁöÑÊåáÊ®ô„ÄÅÊó•Ë™åËàáËøΩËπ§ÂÖ®Ë¶ÜËìã„ÄÇ
- **Â§öË™ûË®ÄÊîØÊè¥**: Â±ïÁ§∫Â¶Ç‰ΩïÁõ£Êéß‰ª•‰∏çÂêåË™ûË®ÄÔºàJava„ÄÅPython„ÄÅGoÔºâÁ∑®ÂØ´ÁöÑÊáâÁî®Á®ãÂºè„ÄÇ
- **‰∫íÂãïÂºèÈ´îÈ©ó**: Êèê‰æõ‰∏ÄÂÄã Streamlit ÊáâÁî®Á®ãÂºèÔºåÁî®ÊñºÂç≥ÊôÇÁõ£Êéß„ÄÅÊÉÖÂ¢ÉÊ®°Êì¨ËàáË≥áÊñôÊé¢Á¥¢„ÄÇ
- **ÊúÄ‰Ω≥ÂØ¶Ë∏ê**: ÊáâÁî®ÂèØËßÄÊ∏¨ÊÄßË®≠Ë®àÊ®°ÂºèÔºåÂ¶ÇÈõÜ‰∏≠Êî∂ÈõÜËàáÁµêÊßãÂåñÊó•Ë™å„ÄÇ
""",
        "ready_message": "Â∞àÊ°àÂ∑≤Ê∫ñÂÇôÂ∞±Á∑íÔºÅÊâÄÊúâÊúçÂãôÊáâÂ∑≤Âú® Docker ‰∏≠ÈÅãË°å„ÄÇ",
        "before_you_begin_header": "‚ú® ÈñãÂßã‰πãÂâç",
        "before_you_begin_content": "Ë´ãÁ¢∫‰øùÊÇ®Â∑≤Èñ±ËÆÄÂ∞àÊ°àÁöÑ `README.md` ‰∏¶Â∑≤Âü∑Ë°å `docker-compose up -d` ‰æÜÂïüÂãïÊâÄÊúâÊúçÂãô„ÄÇ",
        "language_select": "ÈÅ∏ÊìáË™ûË®ÄÔºö",

        # 1_Architecture.py
        "arch_title": "üèóÔ∏è Á≥ªÁµ±Êû∂Êßã",
        "arch_overview_header": "ÂèØËßÄÊ∏¨ÊÄßÊû∂ÊßãÁ∏ΩË¶Ω",
        "arch_principles_header": "üéØ ÈóúÈçµÊû∂ÊßãÂéüÂâá",
        "arch_principle_central": "ÈõÜ‰∏≠ÂåñÊî∂ÈõÜ",
        "arch_principle_central_desc": "‰ΩøÁî®ÂñÆ‰∏Ä OpenTelemetry Collector Êî∂ÈõÜÊâÄÊúâÈÅôÊ∏¨Ë≥áÊñô",
        "arch_principle_vendor": "Âª†ÂïÜ‰∏≠Á´ãÊÄß",
        "arch_principle_vendor_desc": "Âü∫ÊñºÊ®ôÊ∫ñÁöÑ OpenTelemetry ÂØ¶‰Ωú",
        "arch_principle_scale": "ÂèØÊì¥Â±ïÊÄß",
        "arch_principle_scale_desc": "ÂÖ∑ÂÇôÊåÅ‰πÖÂåñÂÑ≤Â≠òÁöÑÊ∞¥Âπ≥ÂèØÊì¥Â±ïÂÖÉ‰ª∂",
        "arch_principle_security": "ÂÆâÂÖ®ÊÄß",
        "arch_principle_security_desc": "ÂèØÈÖçÁΩÆÁöÑË™çË≠âËàáÂä†ÂØÜÊîØÊè¥",
        "arch_principle_ha": "È´òÂèØÁî®ÊÄß",
        "arch_principle_ha_desc": "ÂÖ∑ÂÇôÂÅ•Â∫∑Ê™¢Êü•ÂíåËá™ÂãïÂæ©ÂéüÁöÑÂΩàÊÄßË®≠Ë®à",
        "data_flow_header": "üåä Ë≥áÊñôÊµÅÊû∂Êßã",
        "perf_header": "‚ö° ÊïàËÉΩÁâπÊÄß",
        "perf_component": "ÂÖÉ‰ª∂",
        "perf_throughput": "È†êÊúüÂêûÂêêÈáè",
        "perf_latency": "Âª∂ÈÅ≤ (P95)",
        "perf_memory": "Ë®òÊÜ∂È´î‰ΩøÁî®Èáè",

        # 0_Tutorial_Introduction.py
        "intro_title": "ÊïôÂ≠∏Á∞°‰ªã",
        "intro_markdown": """
Êú¨‰∫íÂãïÂºèÊïôÂ≠∏Êó®Âú®ÂºïÂ∞éÊÇ®ÂÆåÊàê **Kafka APM ËàáÂèØËßÄÊ∏¨ÊÄßËß£Ê±∫ÊñπÊ°àÁ§∫ÁØÑ**„ÄÇ
ÊÇ®Â∞áÂ≠∏ÁøíÁ≥ªÁµ±Êû∂Êßã„ÄÅÊäÄË°ìÂ†ÜÁñäÔºå‰ª•ÂèäÂ¶Ç‰Ωï‰ΩøÁî®Áõ£ÊéßÂ∑•ÂÖ∑‰æÜÊéíËß£ÊïàËÉΩÂïèÈ°å„ÄÇ
""",
        "intro_info": "Ë´ã‰æùÂ∫èÁÄèË¶ΩÂÅ¥ÈÇäÊ¨Ñ‰∏≠ÁöÑÈ†ÅÈù¢‰ª•ÂÆåÊàêÊïôÂ≠∏„ÄÇÊàëÂÄëÂª∫Ë≠∞Âæû„ÄåÊû∂Êßã„ÄçÈ†ÅÈù¢ÈñãÂßã„ÄÇ",
        "intro_structure_header": "ÊïôÂ≠∏ÁµêÊßã",
        "intro_structure_content": """
- **Êû∂Êßã**: ‰∫ÜËß£Êï¥È´îÁ≥ªÁµ±Ë®≠Ë®à„ÄÇ
- **ÊäÄË°ìÂ†ÜÁñä**: ‰∫ÜËß£ÊâÄ‰ΩøÁî®ÁöÑÊäÄË°ì„ÄÇ
- **Âç≥ÊôÇÂÑÄË°®Êùø**: Êü•Áúã Grafana ÂÑÄË°®ÊùøÁöÑÂØ¶ÈöõÈÅã‰Ωú„ÄÇ
- **Á§∫ÁØÑÊÉÖÂ¢É**: Âü∑Ë°å‰∫íÂãïÂºèÊÉÖÂ¢É‰ª•Ê®°Êì¨ÁúüÂØ¶‰∏ñÁïåÁöÑÊÉÖÊ≥Å„ÄÇ
- **ÈÇÑÊúâÊõ¥Â§ö...**
""",
        "intro_prereq_header": "ÂâçÁΩÆÈúÄÊ±Ç",
        "intro_prereq_content": """
- **Docker**: Á¢∫‰øù Docker Ëàá Docker Compose Â∑≤ÂÆâË£ù‰∏¶Ê≠£Âú®ÈÅãË°å„ÄÇ
- **Â∞àÊ°àÊ™îÊ°à**: ÊÇ®ÊáâÂ∑≤Â∞áÂ∞àÊ°àÊ™îÊ°àË§áË£ΩÂà∞Êú¨Ê©ü„ÄÇ
- **Âü∫Á§é Kafka Áü•Ë≠ò**: ÁÜüÊÇâÂü∫Á§é Kafka Ê¶ÇÂøµÊúÉÊúâÊâÄÂπ´Âä©Ôºå‰ΩÜÈùûÂøÖË¶Å„ÄÇ
""",

        # 2_Tech_Stack.py
        "tech_stack_title": "üîß ÊäÄË°ìÂ†ÜÁñä",
        "tech_app_layer": "üì± ÊáâÁî®Á®ãÂºèÁõ£Ê∏¨Â±§",
        "tech_component": "ÂÖÉ‰ª∂",
        "tech_technology": "ÊäÄË°ì",
        "tech_purpose": "Áî®ÈÄî",
        "tech_features": "‰∏ªË¶ÅÁâπËâ≤",
        "tech_java_apps": "Java ÊáâÁî®Á®ãÂºè",
        "tech_python_apps": "Python ÊáâÁî®Á®ãÂºè",
        "tech_go_apps": "Go ÊáâÁî®Á®ãÂºè",
        "tech_java_agent": "OpenTelemetry Java Agent",
        "tech_python_sdk": "OpenTelemetry Python SDK",
        "tech_go_sdk": "OpenTelemetry Go SDK",
        "tech_auto_instrument": "Ëá™ÂãïÁõ£Ê∏¨",
        "tech_manual_instrument": "ÊâãÂãïÁõ£Ê∏¨",
        "tech_java_features": "Èõ∂Á®ãÂºèÁ¢ºÁõ£Ê∏¨„ÄÅJVM ÊåáÊ®ô„ÄÅKafka ÂÆ¢Êà∂Á´ØËøΩËπ§",
        "tech_python_features": "Ëá™Ë®ÇÊåáÊ®ô„ÄÅÁµêÊßãÂåñÊó•Ë™å„ÄÅÈùûÂêåÊ≠•ÊîØÊè¥",
        "tech_go_features": "È´òÊïàËÉΩËøΩËπ§„ÄÅËá™Ë®ÇÊåáÊ®ô„ÄÅ‰ΩéË≤†Êìî",
        "tech_streaming_platform": "üì® Ë®äÊÅØ‰∏≤ÊµÅÂπ≥Âè∞",
        "tech_kafka": "Apache Kafka",
        "tech_zookeeper": "Zookeeper",
        "tech_jmx": "JMX Exporters",
        "tech_confluent": "Confluent Platform",
        "tech_apache_zookeeper": "Apache Zookeeper",
        "tech_prometheus_jmx": "Prometheus JMX Exporter",
        "tech_broker": "Ë®äÊÅØ‰ª£ÁêÜ‰º∫ÊúçÂô®",
        "tech_coordination": "ÂçîË™øÊúçÂãô",
        "tech_metrics_collection": "ÊåáÊ®ôÊî∂ÈõÜ",
        "tech_kafka_features": "È´òÂêûÂêêÈáè„ÄÅËÄê‰πÖÊÄß„ÄÅÂü∫ÊñºÂàÜÂçÄÁöÑÊì¥Â±ï",
        "tech_zookeeper_features": "Âè¢ÈõÜÂçîË™ø„ÄÅÈÖçÁΩÆÁÆ°ÁêÜ",
        "tech_jmx_features": "‰ª£ÁêÜ‰º∫ÊúçÂô®ÊåáÊ®ô„ÄÅJVM Áµ±Ë®à„ÄÅ‰∏ªÈ°åÊåáÊ®ô",
        "tech_obs_infra": "üëÅÔ∏è ÂèØËßÄÊ∏¨ÊÄßÂü∫Á§éË®≠ÊñΩ",
        "tech_otel_collector": "OpenTelemetry Collector",
        "tech_prometheus": "Prometheus",
        "tech_loki": "Loki",
        "tech_tempo": "Tempo",
        "tech_grafana": "Grafana",
        "tech_otel_contrib": "OTEL Collector Contrib",
        "tech_prometheus_tsdb": "Prometheus TSDB",
        "tech_grafana_loki": "Grafana Loki",
        "tech_grafana_tempo": "Grafana Tempo",
        "tech_grafana_viz": "Grafana",
        "tech_pipeline": "ÈÅôÊ∏¨ÁÆ°ÈÅì",
        "tech_metrics_storage": "ÊåáÊ®ôÂÑ≤Â≠ò",
        "tech_log_aggregation": "Êó•Ë™åËÅöÂêà",
        "tech_trace_storage": "ËøΩËπ§ÂÑ≤Â≠ò",
        "tech_visualization": "Ë¶ñË¶∫Âåñ",
        "tech_otel_features": "Ë≥áÊñôÊî∂ÈõÜ„ÄÅËôïÁêÜ„ÄÅË∑ØÁî±„ÄÅÂåØÂá∫",
        "tech_prometheus_features": "ÊôÇÈñìÂ∫èÂàóË≥áÊñôÂ∫´„ÄÅPromQL Êü•Ë©¢„ÄÅÂëäË≠¶",
        "tech_loki_features": "Êó•Ë™åÁ¥¢Âºï„ÄÅLogQL Êü•Ë©¢„ÄÅÊ®ôÁ±§ÁµÑÁπî",
        "tech_tempo_features": "ÂàÜÊï£ÂºèËøΩËπ§„ÄÅËøΩËπ§ÈóúËÅØ„ÄÅÂèñÊ®£",
        "tech_grafana_features": "ÂÑÄË°®Êùø„ÄÅÂëäË≠¶„ÄÅË≥áÊñôÊé¢Á¥¢",

        # 4_Demo_Scenarios.py
        "scenarios_title": "üéÆ ‰∫íÂãïÂºèÁ§∫ÁØÑÊÉÖÂ¢É",
        "scenarios_header": "ÂãïÊâãÂ≠∏ÁøíÊÉÖÂ¢É",
        "choose_scenario": "üéØ ÈÅ∏ÊìáÂ≠∏ÁøíÊÉÖÂ¢É",
        "select_scenario": "ÈÅ∏ÊìáÊÉÖÂ¢É:",
        "scenario_a_title": "ÊÉÖÂ¢É A: Ê≠£Â∏∏ÁáüÈÅãÁõ£Êéß",
        "scenario_b_title": "ÊÉÖÂ¢É B: ÊïàËÉΩÊïÖÈöúÊéíÈô§",
        "scenario_c_title": "ÊÉÖÂ¢É C: ÊïÖÈöúÂæ©Âéü",
        "scenario_d_title": "ÊÉÖÂ¢É D: Êì¥Â±ïÁáüÈÅã",

        "scenario_a_header": "üéØ ÊÉÖÂ¢É A: Ê≠£Â∏∏ÁáüÈÅãÁõ£Êéß",
        "scenario_a_objectives_header": "Â≠∏ÁøíÁõÆÊ®ô",
        "scenario_a_objectives_content": """
1.  üöÄ ÂïüÂãïÂÆåÊï¥ÁöÑÂèØËßÄÊ∏¨ÊÄßÁí∞Â¢É
2.  üìä ‰∫ÜËß£Âü∫Ê∫ñÊïàËÉΩÊåáÊ®ô
3.  üîç Êé¢Á¥¢ Grafana ÂÑÄË°®Êùø
4.  üö® Ë®≠ÂÆöÂü∫Êú¨ÂëäË≠¶Ë¶èÂâá
""",
        "scenario_a_guide_header": "üìù ÈÄêÊ≠•ÊåáÂ∞é",
        "scenario_a_step1_header": "Ê≠•È©ü 1: Áí∞Â¢ÉÂïüÂãï",
        "scenario_a_step1_goal": "**ÁõÆÊ®ô**: ÂïüÂãïÊâÄÊúâÊúçÂãô‰∏¶È©óË≠âÁãÄÊÖã",
        "scenario_a_step1_button": "üöÄ ÂïüÂãïÁí∞Â¢É",
        "scenario_a_step1_spinner": "Ê≠£Âú®ÂïüÂãïÊúçÂãô...",
        "scenario_a_step1_success": "‚úÖ Áí∞Â¢ÉÂïüÂãïÊàêÂäü!",
        "scenario_a_step1_error": "‚ùå ÂïüÂãïÂ§±Êïó:",
        "scenario_a_step1_manual": """
# ÊâãÂãïÂïüÂãïÊåá‰ª§
cd kakfa-o11y
./start.sh

# Êàñ‰ΩøÁî® Docker Compose
docker compose up -d
""",
        "scenario_a_step2_header": "Ê≠•È©ü 2: ÊúçÂãôÁãÄÊÖãÊ™¢Êü•",
        "scenario_a_step2_goal": "**ÁõÆÊ®ô**: Á¢∫Ë™çÊâÄÊúâÊúçÂãôÊ≠£Â∏∏ÈÅãË°å",
        "scenario_a_step2_button": "üîç Ê™¢Êü•ÊúçÂãôÁãÄÊÖã",
        "scenario_a_step2_spinner": "Ê™¢Êü•ÊúçÂãô‰∏≠...",
        "scenario_a_step2_service_col": "ÊúçÂãô",
        "scenario_a_step2_status_col": "ÁãÄÊÖã",
        "scenario_a_step3_header": "Ê≠•È©ü 3: ËßÄÂØüÁîüÁî¢ËÄÖ/Ê∂àË≤ªËÄÖÊ¥ªÂãï",
        "scenario_a_step3_goal": "**ÁõÆÊ®ô**: ËßÄÂØüË®äÊÅØÊµÅÂíåÂü∫Ê∫ñÊåáÊ®ô",
        "scenario_a_step3_producer_button": "üì§ ËßÄÂØü Python Producer",
        "scenario_a_step3_consumer_button": "üì• ËßÄÂØü Go Consumer",
        "scenario_a_step3_spinner": "Áç≤ÂèñÊó•Ë™å...",
        "scenario_a_step4_header": "Ê≠•È©ü 4: Grafana ÂÑÄË°®ÊùøÊé¢Á¥¢",
        "scenario_a_step4_goal": "**ÁõÆÊ®ô**: ÁÜüÊÇâÁõ£Êéß‰ªãÈù¢",
        "scenario_a_step4_access_header": "üìä Â≠òÂèñË≥áË®ä:",
        "scenario_a_step4_url": "- URL: http://localhost:3000",
        "scenario_a_step4_user": "- Â∏≥Ëôü: admin",
        "scenario_a_step4_pass": "- ÂØÜÁ¢º: admin",
        "scenario_a_step4_button": "üîó ÈñãÂïü Grafana",
        "scenario_a_step4_link": "[ÈªûÊìäÈñãÂïü Grafana](http://localhost:3000)",
        "scenario_a_step4_info": "üí° È¶ñÊ¨°ÁôªÂÖ•ÂæåË´ãÊõ¥ÊîπÈ†êË®≠ÂØÜÁ¢º",

        "scenario_b_header": "üîß ÊÉÖÂ¢É B: ÊïàËÉΩÊïÖÈöúÊéíÈô§",
        "scenario_b_objectives_header": "Â≠∏ÁøíÁõÆÊ®ô",
        "scenario_b_objectives_content": """
1.  üêõ ÂºïÂÖ•‰∫∫Â∑•Âª∂ÈÅ≤ÂíåÈåØË™§
2.  üìâ ËßÄÂØüÊåáÊ®ôË°∞ÈÄÄ
3.  üîç ‰ΩøÁî®Êó•Ë™åË≠òÂà•Ê†πÊú¨ÂéüÂõ†
4.  üîó ËøΩËπ§Ë´ãÊ±ÇÊµÅÁ®ã
""",
        "scenario_b_step1_header": "Ê≠•È©ü 1: Ê®°Êì¨Á∂≤Ë∑ØÂïèÈ°å",
        "scenario_b_step1_goal": "**ÁõÆÊ®ô**: Êö´ÂÅú Kafka ÊúçÂãô‰∏¶ËßÄÂØüÂΩ±Èüø",
        "scenario_b_step1_pause_button": "‚è∏Ô∏è Êö´ÂÅú Kafka",
        "scenario_b_step1_pause_spinner": "Êö´ÂÅú Kafka ÊúçÂãô...",
        "scenario_b_step1_pause_success": "‚úÖ Kafka Â∑≤Êö´ÂÅú„ÄÇ",
        "scenario_b_step1_pause_error": "‚ùå Êö´ÂÅú Kafka Â§±Êïó:",
        "scenario_b_step1_unpause_button": "‚èØÔ∏è ÊÅ¢Âæ© Kafka",
        "scenario_b_step1_unpause_spinner": "ÊÅ¢Âæ© Kafka ÊúçÂãô...",
        "scenario_b_step1_unpause_success": "‚úÖ Kafka Â∑≤ÊÅ¢Âæ©„ÄÇ",
        "scenario_b_step1_unpause_error": "‚ùå ÊÅ¢Âæ© Kafka Â§±Êïó:",

        "scenario_c_header": "üí• ÊÉÖÂ¢É C: ÊïÖÈöúÂæ©Âéü",
        "scenario_c_objectives_header": "Â≠∏ÁøíÁõÆÊ®ô",
        "scenario_c_objectives_content": """
1.  üî• Ê®°Êì¨‰ª£ÁêÜ‰º∫ÊúçÂô®ÊïÖÈöú
2.  üîÑ Áõ£ÊéßÂè¢ÈõÜÂæ©Âéü
3.  ‚öñÔ∏è ÂàÜÊûêÊ∂àË≤ªËÄÖÈáçÊñ∞Âπ≥Ë°°
4.  ‚úÖ È©óË≠âË≥áÊñô‰∏ÄËá¥ÊÄß
""",
        "scenario_c_step1_header": "Ê≠•È©ü 1: Ê®°Êì¨‰ª£ÁêÜ‰º∫ÊúçÂô®ÊïÖÈöú",
        "scenario_c_step1_goal": "**ÁõÆÊ®ô**: ÂÅúÊ≠¢ Kafka ÂÆπÂô®‰∏¶ËßÄÂØüÂæ©ÂéüÊÉÖÊ≥Å",
        "scenario_c_step1_stop_button": "üî• ÂÅúÊ≠¢ Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®",
        "scenario_c_step1_stop_spinner": "Ê≠£Âú®ÂÅúÊ≠¢ Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®...",
        "scenario_c_step1_stop_success": "‚úÖ Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®Â∑≤ÂÅúÊ≠¢„ÄÇ",
        "scenario_c_step1_stop_error": "‚ùå ÂÅúÊ≠¢ Kafka Â§±Êïó:",
        "scenario_c_step1_start_button": "‚ôªÔ∏è ÈáçÂïü Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®",
        "scenario_c_step1_start_spinner": "Ê≠£Âú®ÈáçÂïü Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®...",
        "scenario_c_step1_start_success": "‚úÖ Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®Â∑≤ÈáçÂïü„ÄÇ",
        "scenario_c_step1_start_error": "‚ùå ÈáçÂïü Kafka Â§±Êïó:",

        "scenario_d_header": "üìà ÊÉÖÂ¢É D: Êì¥Â±ïÁáüÈÅã",
        "scenario_d_objectives_header": "Â≠∏ÁøíÁõÆÊ®ô",
        "scenario_d_objectives_content": """
1.  üìà Â¢ûÂä†Ë®äÊÅØË≤†Ëºâ
2.  üìä Áõ£ÊéßË≥áÊ∫ê‰ΩøÁî®Áéá
3.  ‚öôÔ∏è ÂãïÊÖãÊì¥Â±ïÊ∂àË≤ªËÄÖ
4.  üöÄ ËßÄÂØüÊïàËÉΩÊîπÂñÑ
""",
        "scenario_d_step1_header": "Ê≠•È©ü 1: Êì¥Â±ïÊ∂àË≤ªËÄÖ",
        "scenario_d_step1_goal": "**ÁõÆÊ®ô**: Â¢ûÂä†Ê∂àË≤ªËÄÖÂØ¶‰æãÁöÑÊï∏Èáè",
        "scenario_d_step1_label": "Python Ê∂àË≤ªËÄÖÊï∏Èáè",
        "scenario_d_step1_button": "‚öôÔ∏è Êì¥Â±ïÊ∂àË≤ªËÄÖ",
        "scenario_d_step1_spinner": "Ê≠£Âú®Êì¥Â±ïÊ∂àË≤ªËÄÖ...",
        "scenario_d_step1_success": "‚úÖ Ê∂àË≤ªËÄÖÊì¥Â±ïÊàêÂäü!",
        "scenario_d_step1_error": "‚ùå Ê∂àË≤ªËÄÖÊì¥Â±ïÂ§±Êïó:",

        # 6_API_Reference.py
        "api_ref_title": "üîó API & Á´ØÈªûÂèÉËÄÉ",
        "api_ref_header": "Ê≠§ËôïÊèê‰æõÂú®Ê≠§Á§∫ÁØÑ‰∏≠‰ΩøÁî®ÁöÑ‰∏ªË¶ÅÊúçÂãôÁ´ØÈªû„ÄÇ",
        "api_ref_obs_header": "üëÅÔ∏è ÂèØËßÄÊ∏¨ÊÄßÊúçÂãô",
        "api_ref_obs_content": """
- **Grafana**: [http://localhost:3000](http://localhost:3000)
  - *‰ΩøÁî®ËÄÖ*: `admin`
  - *ÂØÜÁ¢º*: `grafana`
- **Prometheus**: [http://localhost:9090](http://localhost:9090)
- **Loki**: (ÈÄèÈÅé Grafana Â≠òÂèñ)
- **Tempo**: (ÈÄèÈÅé Grafana Â≠òÂèñ)
""",
        "api_ref_kafka_header": "üì® Kafka ÊúçÂãô",
        "api_ref_kafka_content": """
- **Kafka ‰ª£ÁêÜ‰º∫ÊúçÂô®**: `kafka:9092` (Docker Á∂≤Ë∑ØÂÖßÈÉ®)
- **Zookeeper**: `zookeeper:2181` (Docker Á∂≤Ë∑ØÂÖßÈÉ®)
- **JMX Exporter (Kafka)**: `jmx-exporter-kafka:5556`
- **JMX Exporter (Zookeeper)**: `jmx-exporter-zookeeper:5557`
""",
        "api_ref_otel_header": "üî≠ OpenTelemetry Collector",
        "api_ref_otel_content": """
- **gRPC Á´ØÈªû**: `otel-collector:4317`
- **HTTP Á´ØÈªû**: `otel-collector:4318`
""",

        # 7_Troubleshooting.py
        "troubleshooting_title": "üõ†Ô∏è ÂïèÈ°åÊéíËß£",
        "troubleshooting_header": "üö´ Â∏∏Ë¶ãÂïèÈ°åËàáËß£Ê±∫ÊñπÊ°à",
        "issue_services_fail_header": "ÊúçÂãôÁÑ°Ê≥ïÂïüÂãïÊàñÊåÅÁ∫åÈáçÂïü",
        "issue_services_fail_content": """
- **Ê™¢Êü• Docker Ë≥áÊ∫ê**: Á¢∫‰øù Docker Desktop ÂàÜÈÖç‰∫ÜË∂≥Â§†ÁöÑË®òÊÜ∂È´îÔºàÂª∫Ë≠∞ >8GBÔºâ„ÄÇ
- **Ê™¢Êü•Âü†ËôüË°ùÁ™Å**: Á¢∫‰øù `3000`, `8501`, `9090` Á≠âÂü†ËôüÊú™Ë¢´ÂÖ∂‰ªñÊáâÁî®Á®ãÂºèÂç†Áî®„ÄÇ
- **Êü•ÁúãÊó•Ë™å**: `docker compose logs [service_name]`
- **Ê∏ÖÈô§ËàäÂÆπÂô®**: `docker compose down -v`
""",
        "issue_no_data_header": "Grafana ‰∏≠Ê≤íÊúâË≥áÊñô",
        "issue_no_data_content": """
- **Ê™¢Êü• Prometheus**: ÁÄèË¶Ω `http://localhost:9090` Á¢∫Ë™çÁõÆÊ®ôÊòØÂê¶Ê≠£Â∏∏„ÄÇ
- **Ê™¢Êü•Ë≥áÊñô‰æÜÊ∫ê**: Âú® Grafana ‰∏≠Á¢∫Ë™ç Prometheus Ë≥áÊñô‰æÜÊ∫êË®≠ÂÆöÊ≠£Á¢∫ (`http://prometheus:9090`)„ÄÇ
- **Á≠âÂæÖË≥áÊñôÁî¢Áîü**: ÂïüÂãïÂæåÈúÄË¶ÅÂπæÂàÜÈêòÊôÇÈñìËÆìÊáâÁî®Á®ãÂºèÁî¢ÁîüÈÅôÊ∏¨Ë≥áÊñô„ÄÇ
- **Ê™¢Êü• Collector Êó•Ë™å**: `docker compose logs otel-collector`
""",
        "issue_streamlit_error_header": "Streamlit È†ÅÈù¢ÈåØË™§",
        "issue_streamlit_error_content": """
- **Êü•Áúã Streamlit Êó•Ë™å**: `docker compose logs streamlit-app`
- **Ê™¢Êü•Áõ∏‰æùÂ•ó‰ª∂**: `requirements.txt` ‰∏≠ÁöÑÂ•ó‰ª∂ÊòØÂê¶ÂÆâË£ùÊàêÂäü„ÄÇ
- **ÈáçÂª∫Á´ãÊò†ÂÉèÊ™î**: `docker compose up -d --build streamlit-app`
""",
        "troubleshooting_info": "Â¶ÇÊûúÂïèÈ°å‰ªçÁÑ∂Â≠òÂú®ÔºåË´ãÂú®Â∞àÊ°àÁöÑ GitHub Issues ‰∏≠ÊèêÂá∫„ÄÇ",
    }
} 